{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRrKoXQP-lyp",
        "outputId": "f1687d41-3171-4f43-9e67-51e7a22ae8a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 9 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 0s (12.1 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 120880 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from summa) (1.11.3)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy>=0.19->summa) (1.23.5)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54388 sha256=1676052482d2f6e769006838d8503a583559f5a3dd098303b95c5fdc35c757c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/ca/c5/4958614cfba88ed6ceb7cb5a849f9f89f9ac49971616bc919f\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n",
            "Collecting bert-extractive-summarizer\n",
            "  Downloading bert_extractive_summarizer-0.10.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from bert-extractive-summarizer) (4.35.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from bert-extractive-summarizer) (1.2.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from bert-extractive-summarizer) (3.6.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bert-extractive-summarizer) (3.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (0.19.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (0.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers->bert-extractive-summarizer) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers->bert-extractive-summarizer) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->bert-extractive-summarizer) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->bert-extractive-summarizer) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy->bert-extractive-summarizer) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy->bert-extractive-summarizer) (2.1.3)\n",
            "Installing collected packages: bert-extractive-summarizer\n",
            "Successfully installed bert-extractive-summarizer-0.10.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-m97ckhwk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /tmp/pip-req-build-m97ckhwk\n",
            "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.8.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.2.2)\n",
            "Collecting unidecode (from pke==2.0.0)\n",
            "  Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (0.18.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.3.2)\n",
            "Requirement already satisfied: spacy>=3.2.3 in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pke==2.0.0) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pke==2.0.0) (2023.6.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pke==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.3->pke==2.0.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.3->pke==2.0.0) (0.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.1.3)\n",
            "Building wheels for collected packages: pke\n",
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6160628 sha256=cfddee8f2deb7801a2db79e2be4b68ea7040217b93daaa920d9f9f6a2de91b0f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x_74gyd3/wheels/8c/07/29/6b35bed2aa36e33d77ff3677eb716965ece4d2e56639ad0aab\n",
            "Successfully built pke\n",
            "Installing collected packages: unidecode, pke\n",
            "Successfully installed pke-2.0.0 unidecode-1.3.7\n"
          ]
        }
      ],
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "\n",
        "!pip install transformers\n",
        "!pip install summa\n",
        "\n",
        "!pip install bert-extractive-summarizer\n",
        "\n",
        "# !pip install spacy download en_core_web_sm\n",
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_lg\n",
        "# !pip install textacy\n",
        "\n",
        "!pip install nltk\n",
        "!pip install networkx\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "!pip install git+https://github.com/boudinfl/pke.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5DTLBW5VH2BO"
      },
      "outputs": [],
      "source": [
        "def summaryOutput(body,MaxL,MinL):\n",
        "  os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "  summarizer = pipeline(\"summarization\", model=\"t5-base\",\n",
        "                        tokenizer=\"t5-base\", framework=\"tf\")\n",
        "  summary_text = summarizer(body, max_length=MaxL, min_length=MinL, do_sample=False)[0]['summary_text']\n",
        "\n",
        "  return summary_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NdFD9w2MGfFu"
      },
      "outputs": [],
      "source": [
        "def flowchartMaking(body):\n",
        "    # Generate summary using GPT-2\n",
        "    summarizer = pipeline(\"summarization\", model=\"gpt2\")\n",
        "    summary = summarizer(body, max_length=100, min_length=20, do_sample=False)[0]['summary_text']\n",
        "\n",
        "    # Split summary into sentences\n",
        "    sentences = re.split(r'[.!?]', summary)\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "    # Create the graph\n",
        "    graph = pydot.Dot(graph_type='digraph')\n",
        "\n",
        "    # Define symbols and colors\n",
        "    decision_shape = 'diamond'\n",
        "    action_shape = 'rectangle'\n",
        "    decision_color = 'lightblue'\n",
        "    action_color = 'lightgreen'\n",
        "\n",
        "    # Create nodes for each sentence\n",
        "    nodes = {}\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        if \"if\" in sentence.lower():\n",
        "            nodes[i] = pydot.Node(sentence, shape=decision_shape, style='filled', fillcolor=decision_color)\n",
        "        else:\n",
        "            nodes[i] = pydot.Node(sentence, shape=action_shape, style='filled', fillcolor=action_color)\n",
        "        graph.add_node(nodes[i])\n",
        "\n",
        "    # Connect the nodes based on their relationships\n",
        "    for i in range(len(sentences) - 1):\n",
        "        graph.add_edge(pydot.Edge(nodes[i], nodes[i + 1]))\n",
        "\n",
        "    # Save and display the graph\n",
        "    graph.write_png('flowchart1.png')\n",
        "    img = mpimg.imread('flowchart1.png')\n",
        "    plt.imshow(img)\n",
        "\n",
        "# def flowchartMaking(body):\n",
        "#     # Extract keywords from the input paragraph\n",
        "#     keyphrases = keywords.keywords(body).split('\\n')\n",
        "\n",
        "#     # Create the graph\n",
        "#     graph = pydot.Dot(graph_type='digraph')\n",
        "\n",
        "#     # Define symbols and colors\n",
        "#     decision_shape = 'diamond'\n",
        "#     action_shape = 'rectangle'\n",
        "#     decision_color = 'lightblue'\n",
        "#     action_color = 'lightgreen'\n",
        "\n",
        "#     # Create nodes for each key phrase\n",
        "#     nodes = {}\n",
        "#     for i, phrase in enumerate(keyphrases):\n",
        "#         if \"if\" in phrase.lower():\n",
        "#             nodes[i] = pydot.Node(phrase, shape=decision_shape, style='filled', fillcolor=decision_color)\n",
        "#         else:\n",
        "#             nodes[i] = pydot.Node(phrase, shape=action_shape, style='filled', fillcolor=action_color)\n",
        "#         graph.add_node(nodes[i])\n",
        "\n",
        "#     # Connect the nodes based on their relationships\n",
        "#     for i in range(len(keyphrases) - 1):\n",
        "#         graph.add_edge(pydot.Edge(nodes[i], nodes[i + 1]))\n",
        "\n",
        "#     # Save and display the graph\n",
        "#     graph.write_png('flowchart1.png')\n",
        "#     img = mpimg.imread('flowchart1.png')\n",
        "#     plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Va9SZfBRHcAE"
      },
      "outputs": [],
      "source": [
        "# ONLY NODES\n",
        "# def create_mindmap(paragraph):\n",
        "#     # Tokenize the paragraph into sentences and then into words\n",
        "#     sentences = sent_tokenize(paragraph)\n",
        "#     words = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "#     # Remove stop words and punctuation\n",
        "#     stop_words = set(stopwords.words('english') + list(punctuation))\n",
        "#     filtered_words = []\n",
        "#     for sent in words:\n",
        "#         filtered_words.append([w.lower() for w in sent if w.lower() not in stop_words])\n",
        "\n",
        "#     # Calculate word frequency and filter out infrequent words\n",
        "#     flat_words = [item for sublist in filtered_words for item in sublist]\n",
        "#     freq_dist = nltk.FreqDist(flat_words)\n",
        "#     important_words = [word for word, freq in freq_dist.items() if freq > 2]\n",
        "\n",
        "#     # Build the graph\n",
        "#     G = nx.Graph()\n",
        "#     for i, sent in enumerate(filtered_words):\n",
        "#         for j, word in enumerate(sent):\n",
        "#             if word in important_words:\n",
        "#                 G.add_node(word)\n",
        "#                 if j > 0:\n",
        "#                     G.add_edge(sent[j-1], word)\n",
        "#                 if j < len(sent) - 1:\n",
        "#                     G.add_edge(word, sent[j+1])\n",
        "#                 if i > 0:\n",
        "#                     prev_sent = filtered_words[i-1]\n",
        "#                     if j < len(prev_sent):\n",
        "#                         G.add_edge(word, prev_sent[j])\n",
        "#                 if i < len(filtered_words) - 1:\n",
        "#                     next_sent = filtered_words[i+1]\n",
        "#                     if j < len(next_sent):\n",
        "#                         G.add_edge(word, next_sent[j])\n",
        "\n",
        "#     # Draw the graph\n",
        "#     pos = nx.spring_layout(G)\n",
        "#     plt.figure(figsize=(10, 10))\n",
        "#     nx.draw(G, pos, with_labels=True, font_size=12, font_weight='bold')\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# NODES AND EDGES\n",
        "def create_mindmap(paragraph):\n",
        "    # Tokenize the paragraph into sentences and then into words\n",
        "    sentences = sent_tokenize(paragraph)\n",
        "    words = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
        "    filtered_words = []\n",
        "    for sent in words:\n",
        "        filtered_words.append([w.lower() for w in sent if w.lower() not in stop_words])\n",
        "\n",
        "    # Calculate word frequency and filter out infrequent words\n",
        "    flat_words = [item for sublist in filtered_words for item in sublist]\n",
        "    freq_dist = nltk.FreqDist(flat_words)\n",
        "    important_words = [word for word, freq in freq_dist.items() if freq > 2]\n",
        "\n",
        "    # Build the graph\n",
        "    G = nx.Graph()\n",
        "    for i, sent in enumerate(filtered_words):\n",
        "        for j, word in enumerate(sent):\n",
        "            if word in important_words:\n",
        "                G.add_node(word)\n",
        "                if j > 0:\n",
        "                    G.add_edge(sent[j-1], word, label='follows')\n",
        "                if j < len(sent) - 1:\n",
        "                    G.add_edge(word, sent[j+1], label='follows')\n",
        "                if i > 0:\n",
        "                    prev_sent = filtered_words[i-1]\n",
        "                    if j < len(prev_sent):\n",
        "                        G.add_edge(word, prev_sent[j], label='related to')\n",
        "                if i < len(filtered_words) - 1:\n",
        "                    next_sent = filtered_words[i+1]\n",
        "                    if j < len(next_sent):\n",
        "                        G.add_edge(word, next_sent[j], label='related to')\n",
        "\n",
        "    # Draw the graph\n",
        "    pos = nx.spring_layout(G)\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8, font_color='blue', font_weight='bold')\n",
        "    nx.draw(G, pos, with_labels=True, font_size=12, font_weight='bold')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# NODES EDGES BUT MESSED UP\n",
        "# def create_mindmap(paragraph):\n",
        "#     # Tokenize the paragraph into sentences and then into words\n",
        "#     sentences = sent_tokenize(paragraph)\n",
        "#     words = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "#     # Remove stop words and punctuation\n",
        "#     stop_words = set(stopwords.words('english') + list(punctuation))\n",
        "#     filtered_words = []\n",
        "#     for sent in words:\n",
        "#         filtered_words.append([w.lower() for w in sent if w.lower() not in stop_words])\n",
        "\n",
        "#     # Identify nouns and adjectives\n",
        "#     tagged_words = []\n",
        "#     for sent in filtered_words:\n",
        "#         tagged_words.append(pos_tag(sent))\n",
        "#     nouns = set([word for sent in tagged_words for word, pos in sent if pos.startswith('N')])\n",
        "#     adjectives = set([word for sent in tagged_words for word, pos in sent if pos.startswith('J')])\n",
        "\n",
        "#     # Build the graph\n",
        "#     G = nx.Graph()\n",
        "#     for i, sent in enumerate(filtered_words):\n",
        "#         for j, word in enumerate(sent):\n",
        "#             if word in nouns or word in adjectives:\n",
        "#                 G.add_node(word)\n",
        "#                 if j > 0:\n",
        "#                     prev_word = sent[j-1]\n",
        "#                     if prev_word in nouns or prev_word in adjectives:\n",
        "#                         G.add_edge(prev_word, word, label='follows')\n",
        "#                     elif prev_word in set(['in', 'of', 'on']):\n",
        "#                         G.add_edge(prev_word, word, label=prev_word)\n",
        "#                 if j < len(sent) - 1:\n",
        "#                     next_word = sent[j+1]\n",
        "#                     if next_word in nouns or next_word in adjectives:\n",
        "#                         G.add_edge(word, next_word, label='follows')\n",
        "#                     elif next_word in set(['in', 'of', 'on']):\n",
        "#                         G.add_edge(word, next_word, label=next_word)\n",
        "#                 if i > 0:\n",
        "#                     prev_sent = filtered_words[i-1]\n",
        "#                     if j < len(prev_sent):\n",
        "#                         prev_word = prev_sent[j]\n",
        "#                         if prev_word in nouns or prev_word in adjectives:\n",
        "#                             G.add_edge(prev_word, word, label='related to')\n",
        "#                 if i < len(filtered_words) - 1:\n",
        "#                     next_sent = filtered_words[i+1]\n",
        "#                     if j < len(next_sent):\n",
        "#                         next_word = next_sent[j]\n",
        "#                         if next_word in nouns or next_word in adjectives:\n",
        "#                             G.add_edge(word, next_word, label='related to')\n",
        "\n",
        "#     # Draw the graph\n",
        "#     pos = nx.circular_layout(G)\n",
        "#     plt.figure(figsize=(10, 10))\n",
        "#     nx.draw_networkx_nodes(G, pos, node_size=1000, node_color='#1f77b4')\n",
        "#     nx.draw_networkx_edges(G, pos, edge_color='#b4b4b4')\n",
        "#     labels = nx.get_edge_attributes(G,'label')\n",
        "#     nx.draw_networkx_edge_labels(G, pos, edge_labels=labels, font_size=12, font_weight='bold')\n",
        "#     nx.draw_network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "me9CiLhYIApB"
      },
      "outputs": [],
      "source": [
        "  def exKeywords(body, limit):\n",
        "    extractor = pke.unsupervised.TopicRank()\n",
        "\n",
        "    # load the content of the document, here document is expected to be a simple\n",
        "    # test string and preprocessing is carried out using spacy\n",
        "    extractor.load_document(input=body, language='en')\n",
        "\n",
        "    # keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n",
        "    # and adjectives (i.e. `(Noun|Adj)*`)\n",
        "    extractor.candidate_selection()\n",
        "\n",
        "    # candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
        "    extractor.candidate_weighting()\n",
        "\n",
        "    # N-best selection, keyphrases contains the 10 highest scored candidates as\n",
        "    # (keyphrase, score) tuples\n",
        "    keyphrases = extractor.get_n_best(n=limit)\n",
        "    return keyphrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cDU6uWi3DTP9"
      },
      "outputs": [],
      "source": [
        "def imageUpload():\n",
        "  # To upload an image and get the text in digital format.\n",
        "  # Upload the image file\n",
        "  print(\"Upload an image: \")\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  # Get the file name and data\n",
        "  file_name, file_data = next(iter(uploaded.items()))\n",
        "  # Convert the file data to a numpy array\n",
        "  img_data = np.frombuffer(file_data, np.uint8)\n",
        "  # Read the image using OpenCV\n",
        "  img = cv2.imdecode(img_data, cv2.IMREAD_COLOR)\n",
        "  # Convert the image to RGB\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  # Apply OCR on the image\n",
        "  body = pytesseract.image_to_string(img)\n",
        "\n",
        "  # Print the extracted text\n",
        "  return body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CaqHAbFaGQWa"
      },
      "outputs": [],
      "source": [
        "def textUpload():\n",
        "\n",
        "  print(\"Enter text: \\n\")\n",
        "  body = input()\n",
        "\n",
        "  # Print the entered text\n",
        "  return body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "geBigutJ_hR1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14bba591-0006-44b7-a088-0ebef12ba9b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter\n",
            "1 to upload an Image\n",
            "2 to enter text\n",
            "2\n",
            "Enter text: \n",
            "\n",
            "hello my name is greha what is your name?\n",
            "\n",
            "Your text is: \n",
            "hello my name is greha what is your name?\n",
            "Enter\n",
            "1 for summary\n",
            "2 for Flowchart\n",
            "3 for Mind-Maps\n",
            "4 for Keywords\n",
            "3\n",
            "Output: \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import cv2\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "import os\n",
        "\n",
        "from summarizer import TransformerSummarizer\n",
        "import pydot\n",
        "import re\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "from summa import keywords\n",
        "\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pke\n",
        "\n",
        "n = int(input(\"Enter\\n1 to upload an Image\\n2 to enter text\\n\"))\n",
        "if n == 1:\n",
        "  body = imageUpload()\n",
        "  print(\"\\nYour Extracted text from the image is: \")\n",
        "  print(body)\n",
        "elif n == 2:\n",
        "  body = textUpload()\n",
        "  print(\"\\nYour text is: \")\n",
        "  print(body)\n",
        "\n",
        "kv = int(input(\"Enter\\n1 for summary\\n2 for Flowchart\\n3 for Mind-Maps\\n4 for Keywords\\n\"))\n",
        "if kv == 1:\n",
        "  MaxL = int(input(\"Enter the Maximum limit of summary: \"))\n",
        "  MinL = int(input(\"Enter the Minimum limit of summary: \"))\n",
        "  final = summaryOutput(body,MaxL,MinL)\n",
        "  print(\"Output: \\n\")\n",
        "  start = 0\n",
        "  N = 10\n",
        "  l = final.split()\n",
        "  for stop in range(N, len(l)+N, N):\n",
        "      print(' '.join(l[start:stop]))\n",
        "      start = stop\n",
        "\n",
        "\n",
        "if kv == 2:\n",
        "  print(\"Output: \\n\")\n",
        "  final = flowchartMaking(body)\n",
        "if kv == 3:\n",
        "  print(\"Output: \\n\")\n",
        "  graph = create_mindmap(body)\n",
        "if kv == 4:\n",
        "  limit = int(input(\"Enter Keyword Limit: \"))\n",
        "  final = exKeywords(body, limit)\n",
        "  print(\"Output: \\n\")\n",
        "  # print(final)\n",
        "  start = 0\n",
        "  N = 10\n",
        "  l = final.split()\n",
        "  for stop in range(N, len(l)+N, N):\n",
        "      print(' '.join(l[start:stop]))\n",
        "      start = stop\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "fmSJUu91baBa"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}